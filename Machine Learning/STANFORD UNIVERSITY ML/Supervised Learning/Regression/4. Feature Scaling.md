## Gradient Descent in Practice I - Feature Scaling
We can speed up gradient descent by having each of our input values in roughly the same range. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. <br>
The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally: <br>
![image](https://user-images.githubusercontent.com/92245436/147384701-114327a7-6b11-4d69-804c-a13eb76fdc67.png)

These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.

Two techniques to help with this are **feature scaling** and **mean normalization**. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:
![image](https://user-images.githubusercontent.com/92245436/147384731-0a921948-1dfb-4155-a754-516d343e699c.png)


## Gradient Descent in Practice II - Learning Rate
**Debugging gradient descent** :  Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α. <br>
**Automatic convergence test** :Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as 10^{-3} . However in practice it's difficult to choose this threshold value.<br>

![image](https://user-images.githubusercontent.com/92245436/147384902-fa1be6b6-76b8-4c92-9c9f-f5bfdaedd9c2.png)

It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration. <br>

![image](https://user-images.githubusercontent.com/92245436/147384907-c5f4067c-a15f-436c-9999-8e96e3232a4f.png)


To summarize:
- If α is too small: slow convergence. 
- If α is too large: ￼may not decrease on every iteration and thus may not converge.
